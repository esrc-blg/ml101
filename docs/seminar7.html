<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Tree Based Models | Introduction to Data Science</title>
  <meta name="description" content="Chapter 7 Tree Based Models | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Tree Based Models | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Tree Based Models | Introduction to Data Science" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="seminar6.html">
<link rel="next" href="seminar8.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="lib/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="js/codefolding.js"></script>


<script>
$(document).ready(function () {
  window.initializeCodeFolding();
});
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="lib\bootstrap\3.3.7\css\bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="css\readthedocs.css" type="text/css" />
<link rel="stylesheet" href="css\custom.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this course</a></li>
<li class="chapter" data-level="1" data-path="seminar1.html"><a href="seminar1.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="seminar1.html"><a href="seminar1.html#learning-objectives"><i class="fa fa-check"></i><b>1.1</b> Learning objectives</a><ul>
<li class="chapter" data-level="1.1.1" data-path="seminar1.html"><a href="seminar1.html#dplyr-package"><i class="fa fa-check"></i><b>1.1.1</b> Dplyr package</a></li>
<li class="chapter" data-level="1.1.2" data-path="seminar1.html"><a href="seminar1.html#visualising-a-relationship-bw-two-continuous-variables"><i class="fa fa-check"></i><b>1.1.2</b> Visualising a relationship b/w two continuous variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="seminar2.html"><a href="seminar2.html"><i class="fa fa-check"></i><b>2</b> Classification</a><ul>
<li class="chapter" data-level="2.1" data-path="seminar2.html"><a href="seminar2.html#seminar"><i class="fa fa-check"></i><b>2.1</b> Seminar</a><ul>
<li class="chapter" data-level="2.1.1" data-path="seminar2.html"><a href="seminar2.html#the-non-western-foreigners-data-set"><i class="fa fa-check"></i><b>2.1.1</b> The Non-Western Foreigners Data Set</a></li>
<li class="chapter" data-level="2.1.2" data-path="seminar2.html"><a href="seminar2.html#logistic-regression"><i class="fa fa-check"></i><b>2.1.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="2.1.3" data-path="seminar2.html"><a href="seminar2.html#the-logit-model"><i class="fa fa-check"></i><b>2.1.3</b> The logit model</a></li>
<li class="chapter" data-level="2.1.4" data-path="seminar2.html"><a href="seminar2.html#predict-outcomes-from-logit"><i class="fa fa-check"></i><b>2.1.4</b> Predict Outcomes from logit</a></li>
<li class="chapter" data-level="2.1.5" data-path="seminar2.html"><a href="seminar2.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1.5</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="2.1.6" data-path="seminar2.html"><a href="seminar2.html#model-the-underlying-continuous-process"><i class="fa fa-check"></i><b>2.1.6</b> Model the Underlying Continuous Process</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="seminar3.html"><a href="seminar3.html"><i class="fa fa-check"></i><b>3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="3.1" data-path="seminar3.html"><a href="seminar3.html#seminar-1"><i class="fa fa-check"></i><b>3.1</b> Seminar</a><ul>
<li class="chapter" data-level="3.1.1" data-path="seminar3.html"><a href="seminar3.html#the-validation-set-approach"><i class="fa fa-check"></i><b>3.1.1</b> The Validation Set Approach</a></li>
<li class="chapter" data-level="3.1.2" data-path="seminar3.html"><a href="seminar3.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>3.1.2</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="3.1.3" data-path="seminar3.html"><a href="seminar3.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> k-Fold Cross-Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="seminar4.html"><a href="seminar4.html"><i class="fa fa-check"></i><b>4</b> Subset Selection</a><ul>
<li class="chapter" data-level="4.1" data-path="seminar4.html"><a href="seminar4.html#seminar-2"><i class="fa fa-check"></i><b>4.1</b> Seminar</a><ul>
<li class="chapter" data-level="4.1.1" data-path="seminar4.html"><a href="seminar4.html#subset-selection-methods"><i class="fa fa-check"></i><b>4.1.1</b> Subset Selection Methods</a></li>
<li class="chapter" data-level="4.1.2" data-path="seminar4.html"><a href="seminar4.html#best-subset-selection"><i class="fa fa-check"></i><b>4.1.2</b> Best Subset Selection</a></li>
<li class="chapter" data-level="4.1.3" data-path="seminar4.html"><a href="seminar4.html#forward-and-backward-stepwise-selection"><i class="fa fa-check"></i><b>4.1.3</b> Forward and Backward Stepwise Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="seminar5.html"><a href="seminar5.html"><i class="fa fa-check"></i><b>5</b> Regularisation</a><ul>
<li class="chapter" data-level="5.1" data-path="seminar5.html"><a href="seminar5.html#seminar-3"><i class="fa fa-check"></i><b>5.1</b> Seminar</a><ul>
<li class="chapter" data-level="5.1.1" data-path="seminar5.html"><a href="seminar5.html#ridge-regression-and-the-lasso"><i class="fa fa-check"></i><b>5.1.1</b> Ridge Regression and the Lasso</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="seminar6.html"><a href="seminar6.html"><i class="fa fa-check"></i><b>6</b> All non-linear (polynomials to splines)</a><ul>
<li class="chapter" data-level="6.1" data-path="seminar6.html"><a href="seminar6.html#seminar-4"><i class="fa fa-check"></i><b>6.1</b> Seminar</a><ul>
<li class="chapter" data-level="6.1.1" data-path="seminar6.html"><a href="seminar6.html#polynomial-regression"><i class="fa fa-check"></i><b>6.1.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="6.1.2" data-path="seminar6.html"><a href="seminar6.html#step-functions"><i class="fa fa-check"></i><b>6.1.2</b> Step Functions</a></li>
<li class="chapter" data-level="6.1.3" data-path="seminar6.html"><a href="seminar6.html#splines"><i class="fa fa-check"></i><b>6.1.3</b> Splines</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="seminar7.html"><a href="seminar7.html"><i class="fa fa-check"></i><b>7</b> Tree Based Models</a><ul>
<li class="chapter" data-level="7.1" data-path="seminar7.html"><a href="seminar7.html#seminar-5"><i class="fa fa-check"></i><b>7.1</b> Seminar</a><ul>
<li class="chapter" data-level="7.1.1" data-path="seminar7.html"><a href="seminar7.html#classification-trees"><i class="fa fa-check"></i><b>7.1.1</b> Classification Trees</a></li>
<li class="chapter" data-level="7.1.2" data-path="seminar7.html"><a href="seminar7.html#regression-trees"><i class="fa fa-check"></i><b>7.1.2</b> Regression Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="seminar8.html"><a href="seminar8.html"><i class="fa fa-check"></i><b>8</b> Simulation and Monte Carlos</a><ul>
<li class="chapter" data-level="8.1" data-path="seminar8.html"><a href="seminar8.html#seminar-6"><i class="fa fa-check"></i><b>8.1</b> Seminar</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-models" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Tree Based Models</h1>
<div id="seminar-5" class="section level2">
<h2><span class="header-section-number">7.1</span> Seminar</h2>
<p>Tree models are non-parametric models. Depending on the data generation process, these models can be better predictive models than generalised linear models even with regularisation. We will start with the highly variable simple tree, followed by pruning, the random forrest model and boosting machines.</p>
<p>We load post-election survey data from the 2004 British Election Survey. The data is available <a href="http://philippbroniecki.github.io/ML2017.io/data/bes.dta">here</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># clear workspace</span>
<span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())

<span class="co"># needed because .dta is a foreign file format (STATA format)</span>
<span class="kw">library</span>(readstata13)
bes &lt;-<span class="st"> </span><span class="kw">read.dta13</span>(<span class="st">&quot;bes.dta&quot;</span>)

<span class="co"># drop missing values</span>
bes &lt;-<span class="st"> </span><span class="kw">na.omit</span>(bes)

<span class="co"># drop id variable</span>
bes<span class="op">$</span>cs_id &lt;-<span class="st"> </span><span class="ot">NULL</span></code></pre>
<p>We clean the <code>in_school</code> variable which should be binary indicating whether a respondent is attending school or not. However, we estimated missing values (which is a superior treatment of missing values than list-wise deletion) and forgot classify those predictions into 0s and 1s.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># clean in_school</span>
<span class="kw">table</span>(bes<span class="op">$</span>in_school)</code></pre>
<pre><code>
 -0.405100243979883  -0.286622836951644 -0.0932005119161492 
                  1                   1                   1 
  -0.08278915151733                   0  0.0403350016659423 
                  1                4120                   1 
  0.123419680101826   0.247478125358543                   1 
                  1                   1                  34 </code></pre>
<p>We use the <code>ifelse()</code> function to classify into 0 and 1.</p>
<pre class="sourceCode r"><code class="sourceCode r">bes<span class="op">$</span>in_school &lt;-<span class="st"> </span><span class="kw">ifelse</span> (bes<span class="op">$</span>in_school <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.5</span>, <span class="dv">0</span>, bes<span class="op">$</span>in_school)
<span class="kw">table</span>(bes<span class="op">$</span>in_school)</code></pre>
<pre><code>
   0    1 
4127   34 </code></pre>
<p>Next, we declare the categorical variables in the dataset to be factors en bulk.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># data manipulation</span>
categcorical &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Turnout&quot;</span>, <span class="st">&quot;Vote2001&quot;</span>, <span class="st">&quot;Gender&quot;</span>, <span class="st">&quot;PartyID&quot;</span>, <span class="st">&quot;Telephone&quot;</span>, <span class="st">&quot;edu15&quot;</span>,
                  <span class="st">&quot;edu16&quot;</span>, <span class="st">&quot;edu17&quot;</span>, <span class="st">&quot;edu18&quot;</span>, <span class="st">&quot;edu19plus&quot;</span>, <span class="st">&quot;in_school&quot;</span>, <span class="st">&quot;in_uni&quot;</span>)
<span class="co"># declare factor variables</span>
bes[, categcorical] &lt;-<span class="st"> </span><span class="kw">lapply</span>(bes[, categcorical], factor)</code></pre>
<div id="classification-trees" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Classification Trees</h3>
<p>We use trees to classifyrespondents into voters and non-voters. Here we need the <code>tree</code> package which we have to install if is not already <code>install.packages("tree")</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tree)

<span class="co"># build classification tree (- in formula language means except)</span>
t1 &lt;-<span class="st"> </span><span class="kw">tree</span>( Turnout <span class="op">~</span><span class="st"> </span>. <span class="op">-</span>CivicDutyScores, <span class="dt">data =</span> bes)
<span class="kw">summary</span>(t1)</code></pre>
<pre><code>
Classification tree:
tree(formula = Turnout ~ . - CivicDutyScores, data = bes)
Variables actually used in tree construction:
[1] &quot;CivicDutyIndex&quot; &quot;Vote2001&quot;       &quot;polinfoindex&quot;  
Number of terminal nodes:  6 
Residual mean deviance:  0.8434 = 3504 / 4155 
Misclassification error rate: 0.1769 = 736 / 4161 </code></pre>
<p>We can plot the tree using the standard plot function. On every split a condition is printed. The observations in the left branch are those for which the condition is true and the ones on the right are those for which the condition is false.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot tree object</span>
<span class="kw">plot</span>(t1)
<span class="kw">text</span>(t1, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-136-1.png" width="672" /></p>
<p>We can also examine the splits as text.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># examin the tree object</span>
t1</code></pre>
<pre><code>node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 4161 4763.0 1 ( 0.25931 0.74069 )  
   2) CivicDutyIndex &lt; 19.5 3066 2446.0 1 ( 0.13666 0.86334 )  
     4) Vote2001: 0 243  333.4 1 ( 0.44033 0.55967 ) *
     5) Vote2001: 1 2823 1963.0 1 ( 0.11052 0.88948 )  
      10) CivicDutyIndex &lt; 16.5 1748  950.8 1 ( 0.07723 0.92277 ) *
      11) CivicDutyIndex &gt; 16.5 1075  961.7 1 ( 0.16465 0.83535 ) *
   3) CivicDutyIndex &gt; 19.5 1095 1471.0 0 ( 0.60274 0.39726 )  
     6) Vote2001: 0 429  391.4 0 ( 0.82984 0.17016 ) *
     7) Vote2001: 1 666  918.2 1 ( 0.45646 0.54354 )  
      14) polinfoindex &lt; 5.5 356  483.4 0 ( 0.58427 0.41573 ) *
      15) polinfoindex &gt; 5.5 310  383.7 1 ( 0.30968 0.69032 ) *</code></pre>
<p>Now we use the validation set approach for classification. We split our data and re-grow the tree on the training data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># initialize random number generator</span>
<span class="kw">set.seed</span>(<span class="dv">2</span>)

<span class="co"># training/test split</span>
train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(bes), <span class="dt">size =</span> <span class="kw">as.integer</span>(<span class="kw">nrow</span>(bes)<span class="op">*</span>.<span class="dv">66</span>))
bes.test &lt;-<span class="st"> </span>bes[ <span class="op">-</span>train, ]
turnout.test &lt;-<span class="st"> </span><span class="kw">ifelse</span>( bes<span class="op">$</span>Turnout[<span class="op">-</span>train] <span class="op">==</span><span class="st"> &quot;1&quot;</span>, <span class="dt">yes =</span> <span class="dv">1</span>, <span class="dt">no =</span> <span class="dv">0</span>)

<span class="co"># grow tree on training data</span>
t2 &lt;-<span class="st"> </span><span class="kw">tree</span>( Turnout <span class="op">~</span><span class="st"> </span>. , <span class="dt">data =</span> bes, <span class="dt">subset =</span> train)</code></pre>
<p>We predict outcomes using the <code>predict()</code> function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcomes</span>
t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># confusion matrix</span>
<span class="kw">table</span>(<span class="dt">prediction =</span> t2.pred, <span class="dt">truth =</span> turnout.test)</code></pre>
<pre><code>          truth
prediction   0   1
         0 179  72
         1 186 978</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># percent correctly classified</span>
<span class="kw">mean</span>( t2.pred <span class="op">==</span><span class="st"> </span>turnout.test )</code></pre>
<pre><code>[1] 0.8176678</code></pre>
<p>We correctly classify 82% of the observations. In classification models, the Brier Score is often used as as measure of model quality. We estimate it as the average of the squared differences between predicted probabilities and true outcomes. It is, thus, similar to the MSE.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># using the predict function to predict outcomes from tree</span>
t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;vector&quot;</span>)
<span class="kw">head</span>(t2.pred)</code></pre>
<pre><code>            0         1
1  0.39516129 0.6048387
5  0.04152249 0.9584775
6  0.13796477 0.8620352
7  0.39516129 0.6048387
9  0.04152249 0.9584775
14 0.13796477 0.8620352</code></pre>
<p>Next we estimate the Brier Score.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># the second column of t2.pred is the probabilities that the outcomes is equal to 1</span>
t2.pred &lt;-<span class="st"> </span>t2.pred[,<span class="dv">2</span>]

<span class="co"># brier score</span>
mse.tree &lt;-<span class="st"> </span><span class="kw">mean</span>( (t2.pred <span class="op">-</span><span class="st"> </span>turnout.test)<span class="op">^</span><span class="dv">2</span> )</code></pre>
<p>We turn to cost-complexity pruning to see if we can simplify the tree and thus decrease variance without increasing bias. We use k-fold cross-validation to determine the best size of the tree.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3</span>)
cv.t2 &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(t2, <span class="dt">FUN =</span> prune.misclass)

<span class="co"># illustrate</span>
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))
<span class="kw">plot</span>(cv.t2<span class="op">$</span>size, cv.t2<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)
<span class="kw">plot</span>(cv.t2<span class="op">$</span>k, cv.t2<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-142-1.png" width="672" /></p>
<p>We can prune the tree to four terminal nodes.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># prune the tree (pick the smallest tree that does not substiantially increase error)</span>
prune.t2 &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(t2, <span class="dt">best =</span> <span class="dv">4</span>)
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(prune.t2)
<span class="kw">text</span>(prune.t2, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-143-1.png" width="672" /></p>
<p>We then predict outcomes.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcomes</span>
t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># did we loose predictive power?</span>
<span class="kw">mean</span>( t2.pred <span class="op">==</span><span class="st"> </span>turnout.test )</code></pre>
<pre><code>[1] 0.8176678</code></pre>
<p>Let’s estimate the Brier Score</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Brier score</span>
t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;vector&quot;</span>)[,<span class="dv">2</span>]
mse.pruned &lt;-<span class="st"> </span><span class="kw">mean</span>( (t2.pred <span class="op">-</span><span class="st"> </span>turnout.test)<span class="op">^</span><span class="dv">2</span> ) </code></pre>
<p>We still correctly classify 0.1338879% of the observations and the brier score remained stable. In the previous plots, we saw that we should do worse if we prune back the tree to have less than 4 terminal nodes. We examine what happens if we overdo it.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># using &quot;wrong&quot; value for pruning (where the error rate does increase)</span>
prune.t2 &lt;-<span class="st"> </span><span class="kw">prune.misclass</span>(t2, <span class="dt">best =</span> <span class="dv">2</span>)
<span class="kw">plot</span>(prune.t2)
<span class="kw">text</span>(prune.t2, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-146-1.png" width="672" /></p>
<p>We now predict outcomes based on the tree that is too small.</p>
<pre class="sourceCode r"><code class="sourceCode r">t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)

<span class="co"># our predictive power decreased</span>
<span class="kw">mean</span>( t2.pred <span class="op">==</span><span class="st"> </span>turnout.test )</code></pre>
<pre><code>[1] 0.8007067</code></pre>
<p>Let’s estimate the Brier Score.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># brier score</span>
t2.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.t2, <span class="dt">newdata =</span> bes.test, <span class="dt">type =</span> <span class="st">&quot;vector&quot;</span>)[,<span class="dv">2</span>]
mse.pruned2 &lt;-<span class="st"> </span><span class="kw">mean</span>( (t2.pred <span class="op">-</span><span class="st"> </span>turnout.test)<span class="op">^</span><span class="dv">2</span> ) </code></pre>
<p>We see that our test error increases.</p>
</div>
<div id="regression-trees" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Regression Trees</h3>
<p>We predict the continuous variable <code>Income</code>. The plot of the regression tree is similar. However, in the terminal nodes the mean values of the dependent variable for that group are displayed rather than the class labels.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># grow a regression tree</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)
reg.t1 &lt;-<span class="st"> </span><span class="kw">tree</span>(Income <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> bes, <span class="dt">subset =</span> train)
<span class="kw">summary</span>(reg.t1)</code></pre>
<pre><code>
Regression tree:
tree(formula = Income ~ ., data = bes, subset = train)
Variables actually used in tree construction:
[1] &quot;edu19plus&quot; &quot;Age&quot;       &quot;Telephone&quot;
Number of terminal nodes:  5 
Residual mean deviance:  3.849 = 10550 / 2741 
Distribution of residuals:
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-6.98100 -1.48700  0.01935  0.00000  1.21000  9.21000 </code></pre>
<p>Let’s plot the tree.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot regression tree</span>
<span class="kw">plot</span>(reg.t1)
<span class="kw">text</span>(reg.t1, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>We can also examine the same output as text.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># examin the tree objext</span>
t1</code></pre>
<pre><code>node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 4161 4763.0 1 ( 0.25931 0.74069 )  
   2) CivicDutyIndex &lt; 19.5 3066 2446.0 1 ( 0.13666 0.86334 )  
     4) Vote2001: 0 243  333.4 1 ( 0.44033 0.55967 ) *
     5) Vote2001: 1 2823 1963.0 1 ( 0.11052 0.88948 )  
      10) CivicDutyIndex &lt; 16.5 1748  950.8 1 ( 0.07723 0.92277 ) *
      11) CivicDutyIndex &gt; 16.5 1075  961.7 1 ( 0.16465 0.83535 ) *
   3) CivicDutyIndex &gt; 19.5 1095 1471.0 0 ( 0.60274 0.39726 )  
     6) Vote2001: 0 429  391.4 0 ( 0.82984 0.17016 ) *
     7) Vote2001: 1 666  918.2 1 ( 0.45646 0.54354 )  
      14) polinfoindex &lt; 5.5 356  483.4 0 ( 0.58427 0.41573 ) *
      15) polinfoindex &gt; 5.5 310  383.7 1 ( 0.30968 0.69032 ) *</code></pre>
<p>We estimate test error of our tree.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MSE</span>
mse.tree &lt;-<span class="st"> </span><span class="kw">mean</span>( (bes.test<span class="op">$</span>Income <span class="op">-</span><span class="st"> </span><span class="kw">predict</span>(reg.t1, <span class="dt">newdata =</span> bes.test))<span class="op">^</span><span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre>
<p>We apply pruning again to get a smaller more interpretable tree.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># cross-validation (to determine cutback size for pruning)</span>
cv.reg.t1 &lt;-<span class="st"> </span><span class="kw">cv.tree</span>(reg.t1)
<span class="kw">plot</span>(cv.reg.t1)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-153-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv.reg.t1<span class="op">$</span>size, cv.reg.t1<span class="op">$</span>dev, <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-153-2.png" width="672" /></p>
<p>This is time we will increase error by pruning the tree. We choose four as a smaller tree size that does not increase RSS by much.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pruning</span>
prune.reg.t1 &lt;-<span class="st"> </span><span class="kw">prune.tree</span>(reg.t1, <span class="dt">best =</span> <span class="dv">4</span>)
<span class="kw">plot</span>(prune.reg.t1)
<span class="kw">text</span>(prune.reg.t1, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-154-1.png" width="672" /></p>
<p>We can predict the outcome based on our pruned back tree. We will predict four values because we have four terminal nodes. We can illustrate the groups and their variance and estimate the MSE of our prediction.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcomes</span>
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(prune.reg.t1, <span class="dt">newdata =</span> bes.test)
<span class="kw">plot</span>(yhat, bes.test<span class="op">$</span>Income)
<span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-155-1.png" width="672" /></p>
<p>We estimate the Brier Score (prediction error).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MSE</span>
mse.pruned &lt;-<span class="st"> </span><span class="kw">mean</span>((yhat <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span>)</code></pre>
<div id="bagging-and-random-forests" class="section level4">
<h4><span class="header-section-number">7.1.2.1</span> Bagging and Random Forests</h4>
<p>We now apply bagging and random forests to improve our prediction. Bagging is the idea that the high variance of a single bushy tree can be reduced by bootstapping samples and averaging over trees that were grown on the samples.</p>
<p>Note: Bagging gets an estimate of the test error for free as it always leaves out some observations when a tree is fit. The reported out-of-bag MSE is thus an estimate of test error. We also estimate test error separately on a test set. This is one particular test set, so the test error may vary.</p>
<p>In our run below the OOB MSE may be a better estimate of test error. It is reported to be lower than our test error estimate. We need to install the <code>randomForest</code> package like so: <code>install.packages("randomForest")</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">123</span>)
<span class="kw">library</span>(randomForest)</code></pre>
<pre><code>randomForest 4.6-14</code></pre>
<pre><code>Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estiamte random forrests model (this may take a moment)</span>
bag1 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Income <span class="op">~</span><span class="st"> </span>. , <span class="dt">mtry =</span> <span class="dv">19</span>, <span class="dt">data =</span> bes, <span class="dt">subset =</span> train, <span class="dt">importance =</span> <span class="ot">TRUE</span>)
bag1</code></pre>
<pre><code>
Call:
 randomForest(formula = Income ~ ., data = bes, mtry = 19, importance = TRUE,      subset = train) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 19

          Mean of squared residuals: 3.680305
                    % Var explained: 37.89</code></pre>
<p>We can use the predict function to predict outcomes from our random forests object.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcome, illustrate, MSE</span>
yhat.bag &lt;-<span class="st"> </span><span class="kw">predict</span>(bag1, <span class="dt">newdata =</span> bes.test)
<span class="kw">plot</span>(yhat.bag, bes.test<span class="op">$</span>Income)
<span class="kw">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># line of 1:1 perfect prediction</span></code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-158-1.png" width="672" /></p>
<p>We estimate the MSE in the validation set</p>
<pre class="sourceCode r"><code class="sourceCode r">mse.bag &lt;-<span class="st"> </span><span class="kw">mean</span>( (yhat.bag <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># reduction of error</span>
(mse.bag <span class="op">-</span><span class="st"> </span>mse.tree) <span class="op">/</span><span class="st"> </span>mse.tree</code></pre>
<pre><code>[1] -0.05762335</code></pre>
<p>We reduce the error rate by 5.76<span class="math inline">\(%\)</span> by using bagging. We examine what happens when we reduce the number of trees we grow. The default is 500.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># dcrease the number of trees (defaults to 500)</span>
bag2 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Income <span class="op">~</span><span class="st"> </span>., <span class="dt">mtry =</span> <span class="dv">19</span>, <span class="dt">data =</span> bes, <span class="dt">subset =</span> train, <span class="dt">ntree =</span> <span class="dv">25</span>, <span class="dt">importance =</span> <span class="ot">TRUE</span>)

<span class="co"># predict outcome</span>
yhat.bag2 &lt;-<span class="st"> </span><span class="kw">predict</span>(bag2, <span class="dt">newdata =</span> bes.test)
mse.bag2 &lt;-<span class="st"> </span>( (yhat.bag2 <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span> )</code></pre>
<p>The result is that our rate increases substantially again.</p>
<p>We now apply random forest. The trick is to decorrelate the trees by randomly considering only a subset of variables at every split. We thereby reduce variance further. The number of variables argument <code>mtry</code> is a tuning parameter.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Random Forest: not trying all vars at each split decorrelates the trees</span>
<span class="kw">set.seed</span>(<span class="dv">123</span>)

<span class="co"># we try to find the optimal tuning parameter for the number of variables to use at each split</span>
oob.error &lt;-<span class="st"> </span><span class="ot">NA</span>
val.set.error &lt;-<span class="st"> </span><span class="ot">NA</span>
<span class="cf">for</span> ( idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>){
  rf1 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Income <span class="op">~</span><span class="st"> </span>., <span class="dt">mtry =</span> idx, <span class="dt">data =</span> bes, <span class="dt">subset =</span> train, <span class="dt">importance =</span> <span class="ot">TRUE</span>)
  <span class="co"># record out of bag error</span>
  oob.error[idx] &lt;-<span class="st"> </span>rf1<span class="op">$</span>mse[<span class="kw">length</span>(rf1<span class="op">$</span>mse)]
  <span class="kw">cat</span>(<span class="kw">paste</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>, <span class="st">&quot;Use &quot;</span>, idx, <span class="st">&quot; variables at each split&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>))
  <span class="co"># record validation set error</span>
  val.set.error[idx] &lt;-<span class="st"> </span><span class="kw">mean</span>( (<span class="kw">predict</span>(rf1, <span class="dt">newdata =</span> bes.test) <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span> )
}</code></pre>
<pre><code>
Use 1 variables at each split
Use 2 variables at each split
Use 3 variables at each split
Use 4 variables at each split
Use 5 variables at each split
Use 6 variables at each split
Use 7 variables at each split
Use 8 variables at each split
Use 9 variables at each split
Use 10 variables at each split</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check optimal values for mtry</span>
<span class="kw">matplot</span>( <span class="dv">1</span><span class="op">:</span>idx, <span class="kw">cbind</span>(oob.error, val.set.error), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>),
         <span class="dt">type =</span> <span class="st">&quot;b&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;MSE&quot;</span>, <span class="dt">frame.plot =</span> <span class="ot">FALSE</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;OOB&quot;</span>, <span class="st">&quot;Val. Set&quot;</span>), <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>),
       <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-161-1.png" width="672" /></p>
<p>We use 3 as the optimal value for <code>mtry</code>. In cases where it is hard to decide, it’s a good idea to choose the less complex model.</p>
<pre class="sourceCode r"><code class="sourceCode r">rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Income <span class="op">~</span><span class="st"> </span>., <span class="dt">mtry =</span> <span class="dv">4</span>, <span class="dt">data =</span> bes, <span class="dt">subset =</span> train, <span class="dt">importance =</span> <span class="ot">TRUE</span>)

<span class="co"># predict outcomes</span>
yhat.rf &lt;-<span class="st"> </span><span class="kw">predict</span>(rf, <span class="dt">newdata =</span> bes.test)
mse.rf &lt;-<span class="st"> </span><span class="kw">mean</span>( (yhat.rf <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># on previous random forests model</span>
(mse.rf <span class="op">-</span><span class="st"> </span>mse.bag) <span class="op">/</span><span class="st"> </span>mse.bag</code></pre>
<pre><code>[1] -0.03454119</code></pre>
<p>We reduced the error rate by another 3.45<span class="math inline">\(\%\)</span> by decorrelating the trees. We can exmine variable importance as well. Variable reduction is obtained as the average that a predictor reduces error at splits within a tree where it was used and averaged again over all trees. Similarly, node purity is based on the gini index of how heterogenous a group becomes due to a split.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))
<span class="co"># which varaibles help explain outcome</span>
<span class="kw">importance</span>(rf1)</code></pre>
<pre><code>                  %IncMSE IncNodePurity
Turnout          3.485190     186.58860
Vote2001         9.654237     180.45489
Age             79.593695    3492.19404
Gender           8.343223     281.50559
PartyID          3.053795     207.56146
Influence       13.608173     823.65790
Attention       19.417204     955.05716
Telephone       55.951595     534.66524
LeftrightSelf    7.783832     869.14777
CivicDutyIndex  24.542362     942.35134
polinfoindex    12.441130     867.21040
edu15           10.163296     580.35854
edu16            5.283346     185.01184
edu17            5.651777     112.05356
edu18            2.655123     110.94208
edu19plus       71.813297    3112.61687
in_school       19.692186      80.59582
in_uni          -2.576748      22.41477
CivicDutyScores 25.916963    1684.33658</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># importance plot</span>
<span class="kw">varImpPlot</span>(rf1)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-163-1.png" width="672" /></p>
</div>
<div id="boosting" class="section level4">
<h4><span class="header-section-number">7.1.2.2</span> Boosting</h4>
<p>The general idea of boosting is that a tree is fit to predict outcome. The second tree is then fit on the residual of the first and so with all following trees. Each additional tree is discounted by a learning rate, so that each tree does not contribute much but slowly the ensemble becomes more predictive.</p>
<p>Install the <code>gbm</code> package like so <code>install.packages("gbm")</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm)</code></pre>
<pre><code>Loaded gbm 2.1.5</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</code></pre>
<p>We run gradient boosting. The tuning parameters are the tree size. Tree size is directly related to the second tuning parameter: the learning rate. When the learning rate is smaller, we need more trees. The third tuning parameter interaction.depth determines how bushy the tree is. Common choices are 1, 2, 4, 8. When interaction depth is 1, each tree is a stump. If we increase to two we can get bivariate interactions with 2 splits and so. A final parameter that is related to the complexity of the tree could be minimum number of observations in the terminal node which defaults to 10.</p>
<p>Notice that we just set hyperparameters. We might achieve better predictions by training the boosting model properly (however, this would take very long).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># gradient boosting</span>
gb1 &lt;-<span class="st"> </span><span class="kw">gbm</span>(Income <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> bes[train, ], 
           <span class="dt">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, 
           <span class="dt">n.trees =</span> <span class="dv">5000</span>, 
           <span class="dt">interaction.depth =</span> <span class="dv">4</span>,
           <span class="dt">shrinkage =</span> <span class="fl">0.001</span>)

<span class="kw">summary</span>(gb1)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-165-1.png" width="672" /></p>
<pre><code>                            var     rel.inf
edu19plus             edu19plus 43.60824543
Age                         Age 28.30747893
Telephone             Telephone  7.24463534
polinfoindex       polinfoindex  5.33906762
edu15                     edu15  3.07150723
Attention             Attention  2.56886790
Gender                   Gender  1.56310050
CivicDutyScores CivicDutyScores  1.48574280
in_school             in_school  1.18768575
LeftrightSelf     LeftrightSelf  1.13485329
Turnout                 Turnout  1.03074516
CivicDutyIndex   CivicDutyIndex  0.93773693
edu16                     edu16  0.68203332
edu17                     edu17  0.61758676
Influence             Influence  0.57391921
edu18                     edu18  0.25732928
Vote2001               Vote2001  0.19667081
PartyID                 PartyID  0.16443143
in_uni                   in_uni  0.02836232</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># partial dependence plots</span>
<span class="kw">plot</span>(gb1, <span class="dt">i =</span> <span class="st">&quot;edu19plus&quot;</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-166-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(gb1, <span class="dt">i =</span> <span class="st">&quot;Age&quot;</span>)</code></pre>
<p><img src="ml101_files/figure-html/unnamed-chunk-166-2.png" width="672" /></p>
<p>We predict the test MSE again and compare to our best model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcome</span>
yhat.gb &lt;-<span class="st"> </span><span class="kw">predict</span>(gb1, <span class="dt">newdata =</span> bes.test, <span class="dt">n.trees =</span> <span class="dv">5000</span>)
mse.gb &lt;-<span class="st"> </span><span class="kw">mean</span>( (yhat.gb <span class="op">-</span><span class="st"> </span>bes.test<span class="op">$</span>Income)<span class="op">^</span><span class="dv">2</span> )

<span class="co"># reduction in error</span>
(mse.gb <span class="op">-</span><span class="st"> </span>mse.rf) <span class="op">/</span><span class="st"> </span>mse.rf</code></pre>
<pre><code>[1] -0.03998188</code></pre>
<p>We reduce the error rate again quite a bit.</p>
</div>
<div id="bayesian-additive-regression-trees-barts" class="section level4">
<h4><span class="header-section-number">7.1.2.3</span> Bayesian Additive Regression Trees (BARTs)</h4>
<p>BARTs move regression trees into a Bayesian framework were priors are used on several parameters to reduce some of the problems of over-fitting that boosting and random forests are prone to. We will illustrate a small example here. Before you can run this, <strong><em>64bit JAVA</em></strong> must be installed on your computer. We then need to install <code>install.packages("rJava")</code> and then <code>install.packages("bartMachine")</code>. Installing Java can be tricky and you may need admin rights on your computer.</p>
<p>There are several tuning parameters for the priors that are set to values that work for most applications. Check the documentation if you want to learn more about these. We set the number of trees to grow, the iterations in the Markow-Chain Monte-Carlo estimations to discard and the draws from the posterior distribution. These parameters should also be tested for instance using cross-validation. The algorithm needs some time to run and therefore we pick out-of-the-box values.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">java.parameters =</span> <span class="st">&quot;-Xmx5g&quot;</span>)
<span class="kw">library</span>(bartMachine)</code></pre>
<pre><code>Loading required package: rJava</code></pre>
<pre><code>Loading required package: bartMachineJARs</code></pre>
<pre><code>Loading required package: car</code></pre>
<pre><code>Loading required package: carData</code></pre>
<pre><code>
Attaching package: &#39;car&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:boot&#39;:

    logit</code></pre>
<pre><code>Loading required package: missForest</code></pre>
<pre><code>Loading required package: itertools</code></pre>
<pre><code>Loading required package: iterators</code></pre>
<pre><code>Welcome to bartMachine v1.2.3! You have 4.77GB memory available.

If you run out of memory, restart R, and use e.g.
&#39;options(java.parameters = &quot;-Xmx5g&quot;)&#39; for 5GB of RAM before you call
&#39;library(bartMachine)&#39;.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># vector of covariate names</span>
Xs &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Turnout&quot;</span>, <span class="st">&quot;Vote2001&quot;</span>, <span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Gender&quot;</span>, <span class="st">&quot;PartyID&quot;</span>, <span class="st">&quot;Influence&quot;</span>, <span class="st">&quot;Attention&quot;</span>,
        <span class="st">&quot;Telephone&quot;</span>, <span class="st">&quot;LeftrightSelf&quot;</span>, <span class="st">&quot;CivicDutyScores&quot;</span>, <span class="st">&quot;polinfoindex&quot;</span>, <span class="st">&quot;edu15&quot;</span>, 
        <span class="st">&quot;edu16&quot;</span>, <span class="st">&quot;edu18&quot;</span>, <span class="st">&quot;edu19plus&quot;</span>, <span class="st">&quot;in_school&quot;</span>, <span class="st">&quot;in_uni&quot;</span>)


<span class="co"># run BART</span>
bart1 &lt;-<span class="st"> </span><span class="kw">bartMachine</span>(<span class="dt">X =</span> bes[train, Xs],
                     <span class="dt">y =</span> bes<span class="op">$</span>Income[train],
                     <span class="dt">num_trees =</span> <span class="dv">500</span>,
                     <span class="dt">num_burn_in =</span> <span class="dv">200</span>,
                     <span class="dt">num_iterations_after_burn_in =</span> <span class="dv">1000</span>,
                     <span class="dt">seed =</span> <span class="dv">123</span>)</code></pre>
<pre><code>bartMachine initializing with 500 trees...
bartMachine vars checked...
bartMachine java init...
bartMachine factors created...
bartMachine before preprocess...
bartMachine after preprocess... 29 total features...
bartMachine sigsq estimated...
bartMachine training data finalized...
Now building bartMachine for regression ...Covariate importance prior ON. 
evaluating in sample data...done</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># predict outcomes on the test set</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> bart1, <span class="dt">new_data =</span> bes[<span class="op">-</span>train, Xs])

<span class="co"># Brier Score</span>
mse.bart &lt;-<span class="st"> </span><span class="kw">mean</span>((bes.test<span class="op">$</span>Income <span class="op">-</span><span class="st"> </span>pred)<span class="op">^</span><span class="dv">2</span>)</code></pre>
<p>Let’s compare our final BART prediction to the reigning champion gradient boosting.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># reduction in error</span>
(mse.bart <span class="op">-</span><span class="st"> </span>mse.gb) <span class="op">/</span><span class="st"> </span>mse.gb</code></pre>
<pre><code>[1] -0.01670494</code></pre>
<p>We have reduced the error by another 1.67<span class="math inline">\(\%\)</span>. Not bad… However, keep in mind that we are usig the validation set approach here. A better evaluation would be based on cross-validation or a truly new dataset.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="seminar6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="seminar8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["ml101.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
